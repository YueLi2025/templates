# Files Overview

## 1. `bird23-train-filtered`

Filtered (latest version) BIRD training dataset. This is the core training set after filtering (e.g. by difficulty or quality). It is used as input for canonicalization and template extraction. The format is typically one example per line or a structured text/JSONL-like dump that downstream scripts read.

---

## 2. `train_column_meaning.json`

Stores descriptions for each column in the BIRD training data. Keys are in the form `database|table_name|column_name`; values are natural-language explanations of what the column represents. Used by schema-building scripts (e.g. `build_all_schemas_from_meaning.py`, `append_food_inspection_schema.py`) to infer column types and semantics, and to generate or extend schema CSVs.

---

## 3. `variable_list_bird.csv`

Schema listing for the BIRD databases. Columns include `database`, `table_name`, `column_name`, and `column_type` (e.g. `num`, `string`, `date`, `boolean`). Defines which tables and columns exist per database and their types. Used by `enumerate_template_combinations.py` to generate only valid (table, column) combinations that match each canonical template’s structure and type constraints.

---

## 4. `build_all_schemas_from_meaning.py`

**Type:** Python script 

Reads column metadata (e.g. from `train_column_meaning.json`) and builds or updates database schemas. Produces the schema definitions (e.g. table/column/type) needed for variable lists and canonicalization. Run this to (re)generate schema files that other scripts depend on.

---

## 5. `bird23-train-filtered-canonical.csv`

**Type:** CSV 

Canonicalized version of `bird23-train-filtered`. Each row corresponds to a training example whose SQL has been normalized into a canonical form (e.g. consistent placeholders like `table_name`, `col_name`, `string`, `num`). Produced by the canonicalization pipeline (e.g. `sql_canonicalizer.py`) and used as input for `extract_canonical_templates.py` to derive the set of unique canonical templates and their counts.

---

## 6. `sql_canonicalizer.py`

**Type:** Python script · **Size:** ~25 KB

Implements SQL canonicalization: it takes raw or filtered SQL (e.g. from training data) and converts it into a standardized “canonical” form. Handles things like table/column placeholders, literal types, and structure so that equivalent queries map to the same canonical template. Its output is written into files like `bird23-train-filtered-canonical.csv`.

---


## 7. `bird23_canonical_templates.csv`

**Type:** CSV · **Size:** ~1.1 MB

Catalog of unique canonical SQL templates. Columns: `template_id`, `canonical_sql`, `count`. Each row is one template (with placeholders like `table_name`, `col_name`, `string`, `num`) and the number of training examples that map to it. Generated by `extract_canonical_templates.py` from the canonicalized training CSV. Used for distribution analysis (e.g. power-law fits) and as input to `enumerate_template_combinations.py`. Contains on the order of thousands of templates.

---

## 8. `extract_canonical_templates.py`

**Type:** Python script · **Size:** ~1 KB

Reads the canonicalized training CSV (e.g. `bird23-train-filtered-canonical.csv`), groups by canonical SQL, and outputs a list of unique templates with counts. Writes `bird23_canonical_templates.csv`. Run after canonicalization to (re)build the template catalog.

---

## 9. `enumerate_template_combinations.py`

**Type:** Python script · **Size:** ~9 KB

Enumerates all valid (table, column) combinations for each canonical template and each database. Uses `variable_list_bird.csv` and `bird23_canonical_templates.csv`. Respects the number of tables/columns per template and column types (e.g. AVG/SUM → numeric, WHERE = string → string). Writes either the full `bird23_template_combinations.csv` or a subset such as `bird23_template_combinations_first2.csv` if run with an argument (e.g. `2` for the first two templates).

---

## 10. `bird23_canonical_templates_count_barplot.png`

**Type:** PNG image · **Size:** ~31 KB

Bar chart of template counts: x-axis = template ID, y-axis = count (linear scale). Shows how many training examples fall into each canonical template. Generated by `plot_template_counts.py` from `bird23_canonical_templates.csv`.

---

## 11. `plot_template_counts.py`

**Type:** Python script · **Size:** ~6 KB

Reads `bird23_canonical_templates.csv` and: (1) produces the linear bar plot (`bird23_canonical_templates_count_barplot.png`); (2) produces the log–log plot with power-law fit (`bird23_canonical_templates_count_barplot_loglog.png`); (3) prints two fits (excluding count=1 only vs excluding count=1 and 2), their intercept and exponent, MSE/RMSE, R², chi-squared goodness-of-fit, and loss comparison. Requires `numpy`, `matplotlib`, and `scipy`.

---

## 12. `bird23_template_combinations.csv`

**Type:** CSV · **Size:** ~263 GB

Full enumeration of valid (template_id, db_id, table(s), column(s)) combinations for all canonical templates and all databases. One row per valid instantiation of a template on a database. Produced by `enumerate_template_combinations.py` when run without a template limit. Very large; use `bird23_template_combinations_first2.csv` for small-scale tests.

---

## Pipeline order (quick reference)

1. **BIRD training set:** `bird23-train-filtered`  
2. **Column semantics:** `train_column_meaning.json` → `build_all_schemas_from_meaning.py` → schema/variable lists  
3. **Schema variables:** `variable_list_bird.csv`  
4. **Canonicalization:** `sql_canonicalizer.py` → `bird23-train-filtered-canonical.csv`  
5. **Templates:** `extract_canonical_templates.py` → `bird23_canonical_templates.csv`  
6. **Combinations:** `enumerate_template_combinations.py` → `bird23_template_combinations.csv` or `bird23_template_combinations_first2.csv`  
7. **Plots & stats:** `plot_template_counts.py` → bar plot and log–log plot, plus printed fit parameters and goodness-of-fit.
